{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25567e1f",
   "metadata": {},
   "source": [
    "# PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab2931a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# импорты\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql import functions as F # типо Functions\n",
    "from pyspark.sql import types as T # типо Types\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import IntegerType, FloatType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866e8405",
   "metadata": {},
   "source": [
    "## Основные операции\n",
    "\n",
    "В данном блокноте будем работать с датасетом:\n",
    "* Titanic Huge Dataset - 1M Passengersc: https://www.kaggle.com/datasets/marcpaulo/titanic-huge-dataset-1m-passengers\n",
    "\n",
    "скачиваем файл `huge_1M_titanic.csv` и кладем в удобное место"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f88630c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x26217fb9720>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Инициализация SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5c2780",
   "metadata": {},
   "source": [
    "### Создание DataFrame для источника данных\n",
    "\n",
    "В pandas мы бы просто считали **.csv** файл как DataFrame и работали с ним.\n",
    "\n",
    "Но тут определяется DataFrame для источника данных, источником является **.csv** файл `huge_1M_titanic.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eec5de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('./data/huge_1M_titanic.csv', header=True, inferSchema=False) \n",
    "\n",
    "# inferSchema=True - прочитать файл целиком, чтобы определить типы колонок \n",
    "# файлы могут быть очень большие "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3754ba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# При чтении определении DataFrame к источнику можно задать схему самостоятельно\n",
    "# Пример:\n",
    "# schema = T.StructType([\n",
    "#     T.StructField(\"PassengerId\", T.IntegerType(), True),\n",
    "#     T.StructField(\"Name\", T.StringType(), True),\n",
    "# ])\n",
    "\n",
    "# df = spark.read.csv(\"file.csv\", header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b62986c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "FileScan csv [PassengerId#17,Survived#18,Pclass#19,Name#20,Sex#21,Age#22,SibSp#23,Parch#24,Ticket#25,Fare#26,Cabin#27,Embarked#28] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/dev/mlRes/BigDataTech/seminars/sem_1/data/huge_1M_titanic.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<PassengerId:string,Survived:string,Pclass:string,Name:string,Sex:string,Age:string,SibSp:s...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Посмотреть план источника данных\n",
    "# схема, путь до данных и другие характеристики \n",
    "df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a0a2fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Кол-во партиций (фрагментов изначальных данных)\n",
    "# Каждая партиция будет обрабатываться отдельным ядром \n",
    "\n",
    "\n",
    "# В данном примере файл `huge_1M_titanic.csv` разбит на 12 партиций (фрагментов)\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a20a451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n",
      "root\n",
      " |-- PassengerId: string (nullable = true)\n",
      " |-- Survived: string (nullable = true)\n",
      " |-- Pclass: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- SibSp: string (nullable = true)\n",
      " |-- Parch: string (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: string (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Определить количество строк к df\n",
    "print(df.count())\n",
    "\n",
    "# Определить колонки df\n",
    "print(df.columns)\n",
    "\n",
    "# Определить схему df\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1995e6d5",
   "metadata": {},
   "source": [
    "### Функция `.show()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "376b72e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+------------------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|              Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+------------------+-----+--------+\n",
      "|       1310|       1|     1|Name1310, Miss. S...|female|NULL|    0|    0|SOTON/O2 3101272| 76.76016504643573| NULL|       C|\n",
      "|       1311|       0|     3|Name1311, Col. Su...|  male|29.0|    0|    0|          223596|10.193096706320182| NULL|       S|\n",
      "|       1312|       0|     3|Name1312, Mr. Sur...|  male|20.0|    0|    0|           54636| 12.02941641147422|  C83|       C|\n",
      "|       1313|       0|     3|Name1313, Mr. Sur...|  male|27.0|    0|    0|        PC 17760|13.429447862759872| NULL|       S|\n",
      "|       1314|       0|     3|Name1314, Mr. Sur...|  male|32.0|    0|    0|          364512| 4.840769450167068|  E33|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+------------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Вывести первые 5 строк df\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8169381c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------\n",
      " PassengerId | 1310                 \n",
      " Survived    | 1                    \n",
      " Pclass      | 1                    \n",
      " Name        | Name1310, Miss. S... \n",
      " Sex         | female               \n",
      " Age         | NULL                 \n",
      " SibSp       | 0                    \n",
      " Parch       | 0                    \n",
      " Ticket      | SOTON/O2 3101272     \n",
      " Fare        | 76.76016504643573    \n",
      " Cabin       | NULL                 \n",
      " Embarked    | C                    \n",
      "-RECORD 1---------------------------\n",
      " PassengerId | 1311                 \n",
      " Survived    | 0                    \n",
      " Pclass      | 3                    \n",
      " Name        | Name1311, Col. Su... \n",
      " Sex         | male                 \n",
      " Age         | 29.0                 \n",
      " SibSp       | 0                    \n",
      " Parch       | 0                    \n",
      " Ticket      | 223596               \n",
      " Fare        | 10.193096706320182   \n",
      " Cabin       | NULL                 \n",
      " Embarked    | S                    \n",
      "-RECORD 2---------------------------\n",
      " PassengerId | 1312                 \n",
      " Survived    | 0                    \n",
      " Pclass      | 3                    \n",
      " Name        | Name1312, Mr. Sur... \n",
      " Sex         | male                 \n",
      " Age         | 20.0                 \n",
      " SibSp       | 0                    \n",
      " Parch       | 0                    \n",
      " Ticket      | 54636                \n",
      " Fare        | 12.02941641147422    \n",
      " Cabin       | C83                  \n",
      " Embarked    | C                    \n",
      "-RECORD 3---------------------------\n",
      " PassengerId | 1313                 \n",
      " Survived    | 0                    \n",
      " Pclass      | 3                    \n",
      " Name        | Name1313, Mr. Sur... \n",
      " Sex         | male                 \n",
      " Age         | 27.0                 \n",
      " SibSp       | 0                    \n",
      " Parch       | 0                    \n",
      " Ticket      | PC 17760             \n",
      " Fare        | 13.429447862759872   \n",
      " Cabin       | NULL                 \n",
      " Embarked    | S                    \n",
      "-RECORD 4---------------------------\n",
      " PassengerId | 1314                 \n",
      " Survived    | 0                    \n",
      " Pclass      | 3                    \n",
      " Name        | Name1314, Mr. Sur... \n",
      " Sex         | male                 \n",
      " Age         | 32.0                 \n",
      " SibSp       | 0                    \n",
      " Parch       | 0                    \n",
      " Ticket      | 364512               \n",
      " Fare        | 4.840769450167068    \n",
      " Cabin       | E33                  \n",
      " Embarked    | C                    \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Позволяет делать вертикальный вывод, если строки слишком большие\n",
    "df.show(5, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96add7af",
   "metadata": {},
   "source": [
    "### Функция `.select()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dc1b7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+------------------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|              Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+------------------+-----+--------+\n",
      "|       1310|       1|     1|Name1310, Miss. S...|female|NULL|    0|    0|SOTON/O2 3101272| 76.76016504643573| NULL|       C|\n",
      "|       1311|       0|     3|Name1311, Col. Su...|  male|29.0|    0|    0|          223596|10.193096706320182| NULL|       S|\n",
      "|       1312|       0|     3|Name1312, Mr. Sur...|  male|20.0|    0|    0|           54636| 12.02941641147422|  C83|       C|\n",
      "|       1313|       0|     3|Name1313, Mr. Sur...|  male|27.0|    0|    0|        PC 17760|13.429447862759872| NULL|       S|\n",
      "|       1314|       0|     3|Name1314, Mr. Sur...|  male|32.0|    0|    0|          364512| 4.840769450167068|  E33|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+------------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Вызывается у набора данных, в нашем случае у df\n",
    "\n",
    "# Получить все столбцы\n",
    "df.select(\"*\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78f3ef0",
   "metadata": {},
   "source": [
    "Если не вызывать `show()`, то будет просто построен план вычисления, но оно не будет выполнено"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a684723f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Age: double]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(df.Age) # То есть на данном этапе DataFrame, как результата выполнения df.select(df.Age) еще нет, есть только план для его вычисления"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb89e643",
   "metadata": {},
   "source": [
    "Когда будет вызвана любая из функций: `.count()`, `.collect()`, `.show()`, функции агрегации - будет получен DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "533b68b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+------+\n",
      "| Age|Survived|Pclass|\n",
      "+----+--------+------+\n",
      "|NULL|       1|     1|\n",
      "|29.0|       0|     3|\n",
      "|20.0|       0|     3|\n",
      "|27.0|       0|     3|\n",
      "|32.0|       0|     3|\n",
      "+----+--------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Передавать конкретные имена колонок\n",
    "df.select(\"Age\", \"Survived\", \"Pclass\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45cb822a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<'Age'>\n",
      "+----+------+\n",
      "| Age|Pclass|\n",
      "+----+------+\n",
      "|NULL|     1|\n",
      "|29.0|     3|\n",
      "|20.0|     3|\n",
      "|27.0|     3|\n",
      "|32.0|     3|\n",
      "+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Обращаться к колонкам можно как к полю объекта (через точку): df.Age\n",
    "print(df.Age)\n",
    "\n",
    "df.select(df.Age, df.Pclass).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac76dea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| Age|\n",
      "+----+\n",
      "|NULL|\n",
      "|29.0|\n",
      "|20.0|\n",
      "|27.0|\n",
      "|32.0|\n",
      "+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.col('Age')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def239dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----+----------+\n",
      "|                Name|         upper(Name)| Age|(Age + 10)|\n",
      "+--------------------+--------------------+----+----------+\n",
      "|Name1310, Miss. S...|NAME1310, MISS. S...|NULL|      NULL|\n",
      "|Name1311, Col. Su...|NAME1311, COL. SU...|29.0|      39.0|\n",
      "|Name1312, Mr. Sur...|NAME1312, MR. SUR...|20.0|      30.0|\n",
      "|Name1313, Mr. Sur...|NAME1313, MR. SUR...|27.0|      37.0|\n",
      "|Name1314, Mr. Sur...|NAME1314, MR. SUR...|32.0|      42.0|\n",
      "+--------------------+--------------------+----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Можно сразу применять различные трансформации к колонкам внутри .select()\n",
    "df.select(df.Name, F.upper(df.Name), df.Age, df.Age + 10).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2460585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+------------------+-----+--------+--------------------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|              Fare|Cabin|Embarked|                Name|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+------------------+-----+--------+--------------------+\n",
      "|       1310|       1|     1|Name1310, Miss. S...|female|NULL|    0|    0|SOTON/O2 3101272| 76.76016504643573| NULL|       C|Name1310, Miss. S...|\n",
      "|       1311|       0|     3|Name1311, Col. Su...|  male|29.0|    0|    0|          223596|10.193096706320182| NULL|       S|Name1311, Col. Su...|\n",
      "|       1312|       0|     3|Name1312, Mr. Sur...|  male|20.0|    0|    0|           54636| 12.02941641147422|  C83|       C|Name1312, Mr. Sur...|\n",
      "|       1313|       0|     3|Name1313, Mr. Sur...|  male|27.0|    0|    0|        PC 17760|13.429447862759872| NULL|       S|Name1313, Mr. Sur...|\n",
      "|       1314|       0|     3|Name1314, Mr. Sur...|  male|32.0|    0|    0|          364512| 4.840769450167068|  E33|       C|Name1314, Mr. Sur...|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+------------------+-----+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"*\", df.Name).show(5) # Можно заметить что колонка Name присутствует 2 раза"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61da804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-----------------+-----------------------------+\n",
      "|(Age > 25)|(Age IS NULL)|(Age IS NOT NULL)|((Age >= 25) AND (Age <= 30))|\n",
      "+----------+-------------+-----------------+-----------------------------+\n",
      "|      NULL|         true|            false|                         NULL|\n",
      "|      true|        false|             true|                         true|\n",
      "|     false|        false|             true|                        false|\n",
      "|      true|        false|             true|                         true|\n",
      "|      true|        false|             true|                        false|\n",
      "|     false|        false|             true|                        false|\n",
      "|      NULL|         true|            false|                         NULL|\n",
      "|      true|        false|             true|                         true|\n",
      "|      NULL|         true|            false|                         NULL|\n",
      "|      true|        false|             true|                        false|\n",
      "+----------+-------------+-----------------+-----------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Передавать множество выражений, например:\n",
    "df.select(df.Age > 25, F.isnull(df.Age), F.isnotnull(df.Age), df.Age.between(25, 30)).show(10)\n",
    "\n",
    "# Можно заметить, что это не выборка по нескольким условиям, это выбор всего, но к каждой строке применяются все переданные выражения,\n",
    "# получаем DataFrame с bool значениями"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021ec87e",
   "metadata": {},
   "source": [
    "### Функция `.filter()` and `.where()`\n",
    "\n",
    "Для выбора строк по условию необходимо использовать функцию `.filter()`\n",
    "\n",
    "`.where()` является алиасом для `.filter()`\n",
    "\n",
    "`.filter()` также как и `.select()` строит план вычисления, DF получается только после вызова соответствующих функций "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "07286446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: int, Survived: int, Pclass: int, Name: string, Sex: string, Age: double, SibSp: int, Parch: int, Ticket: string, Fare: double, Cabin: string, Embarked: string]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filter_one = df.filter(df.Survived == 1)\n",
    "df_filter_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4da0c931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+------------------+-----------+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|              Fare|      Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+------------------+-----------+--------+\n",
      "|       1310|       1|     1|Name1310, Miss. S...|female|NULL|    0|    0|SOTON/O2 3101272| 76.76016504643573|       NULL|       C|\n",
      "|       1315|       1|     3|Name1315, Master....|female| 0.0|    0|    0|           29750|14.805817148933464|       NULL|       S|\n",
      "|       1317|       1|     2|Name1317, Miss. S...|female|28.0|    0|    1|          113781|27.950047997073444|       NULL|       S|\n",
      "|       1320|       1|     3|Name1320, Miss. S...|female|23.0|    1|    0|            7534|20.822545765670117|       NULL|       S|\n",
      "|       1322|       1|     3|Name1322, Miss. S...|female|NULL|    1|    0|     S.O./P.P. 3|12.462809150380991|       NULL|       S|\n",
      "|       1323|       1|     1|Name1323, Mr. Sur...|  male|52.0|    0|    0|      A.5. 18509| 36.84335154107275|        E25|       S|\n",
      "|       1327|       1|     2|Name1327, Master....|female| 4.0|    0|    2|      A/5. 10482| 26.71240152233816|       NULL|       S|\n",
      "|       1336|       1|     2|Name1336, Mrs. Su...|female|28.0|    0|    0|          237668|19.973016819809665|       NULL|       S|\n",
      "|       1338|       1|     3|Name1338, Miss. S...|female|NULL|    0|    0|          231945|6.5779501735234485|       NULL|       C|\n",
      "|       1341|       1|     1|Name1341, Mrs. Su...|female|26.0|    1|    0|            2691| 51.32840021424485|C23 C25 C27|       S|\n",
      "|       1343|       1|     1|Name1343, Mrs. Su...|female|NULL|    1|    0|      A/4. 39886| 68.06574759847447|       NULL|       S|\n",
      "|       1345|       1|     1|Name1345, Miss. S...|female|NULL|    0|    0|            5727|               0.0|       NULL|       C|\n",
      "|       1347|       1|     3|Name1347, Miss. S...|female|21.0|    0|    0|          230080|10.983373624020569|       NULL|       Q|\n",
      "|       1349|       1|     2|Name1349, Miss. S...|female|30.0|    1|    0|      C.A. 33595| 18.92347285610561|        E77|       C|\n",
      "|       1350|       1|     3|Name1350, Miss. S...|female|25.0|    0|    0|          364512| 14.88584271640725|       NULL|       Q|\n",
      "|       1354|       1|     1|Name1354, Master....|female| 9.0|    0|    0|          382652| 53.80746354927432|         F4|       S|\n",
      "|       1357|       1|     3|Name1357, Master....|female| 0.0|    2|    1|          364511|18.502999062647305|       NULL|       C|\n",
      "|       1362|       1|     1|Name1362, Mr. Sur...|  male|56.0|    1|    0|          382652| 50.08934734350119|       NULL|       S|\n",
      "|       1365|       1|     3|Name1365, Mrs. Su...|female|19.0|    0|    2|      W./C. 6607|18.968328786471943|       NULL|       S|\n",
      "|       1369|       1|     3|Name1369, Master....|female| 4.0|    2|    1|          349248|14.744513574618756|       NULL|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+------------------+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filter_one.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b5d5c979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "381681\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+------------------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|              Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+------------------+-----+--------+\n",
      "|       1310|       1|     1|Name1310, Miss. S...|female|NULL|    0|    0|SOTON/O2 3101272| 76.76016504643573| NULL|       C|\n",
      "|       1315|       1|     3|Name1315, Master....|female| 0.0|    0|    0|           29750|14.805817148933464| NULL|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+------------------+-----+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Получается, что когда мы обращаемся к df_filter_one для вызова какой-либо функции, по построенному плану вычислений собирается DataFrame\n",
    "print(df_filter_one.count())\n",
    "\n",
    "df_filter_one.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98378fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если мы примерно оцениваем объем данных и понимаем, что они смогут поместиться к нам в память драйвера, можно конвертировать в pandas DataFrame и работать с ним\n",
    "\n",
    "# Теперь в result_pdf будет находится DataFrame pandas\n",
    "result_pdf = df_filter_one.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ccee3f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "del result_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "25224b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas time:\n",
      "CPU times: total: 62.5 ms\n",
      "Wall time: 62.9 ms\n",
      "\n",
      "PySpark time:\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 300 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "381681"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Можно заметить разницу в Wall time для выполнения функции count() у pandas и PySpark\n",
    "print(\"Pandas time:\")\n",
    "%time result_pdf.count()\n",
    "\n",
    "print(\"\\nPySpark time:\")\n",
    "%time df_filter_one.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "da4690cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------+-----------------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|    Ticket|             Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------+-----------------+-----+--------+\n",
      "|       1323|       1|     1|Name1323, Mr. Sur...|  male|52.0|    0|    0|A.5. 18509|36.84335154107275|  E25|       S|\n",
      "|       1362|       1|     1|Name1362, Mr. Sur...|  male|56.0|    1|    0|    382652|50.08934734350119| NULL|       S|\n",
      "|       1376|       1|     2|Name1376, Mrs. Su...|female|36.0|    0|    0|    347088|16.68499978990871| NULL|       S|\n",
      "|       1384|       1|     1|Name1384, Mrs. Su...|female|35.0|    0|    1|    226875|              0.0| NULL|       C|\n",
      "|       1389|       1|     1|Name1389, Mr. Sur...|  male|33.0|    0|    0|C.A. 29178|33.66723594458008|  A10|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------+-----------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Также можно выполнять фильтрацию по нескольким условиям\n",
    "# \"&\" - и, \"|\" - или\n",
    "df_filter_two = df.filter((df.Age > 30) & (df.Survived == 1))\n",
    "df_filter_two.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387f16f7",
   "metadata": {},
   "source": [
    "`.where()` выполняется аналогично `.filter()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c57d23",
   "metadata": {},
   "source": [
    "### Объединение выбора и фильтрации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3e767253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "| Age|                Name|\n",
      "+----+--------------------+\n",
      "|29.0|Name1311, Col. Su...|\n",
      "|29.0|Name1325, Mr. Sur...|\n",
      "|29.0|Name1469, Miss. S...|\n",
      "|29.0|Name1555, Mrs. Su...|\n",
      "|29.0|Name1556, Mr. Sur...|\n",
      "+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Например, мы хотим сделать выбор строк по условию\n",
    "df.filter(df.Age == 29).select(df.Age, \"Name\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d02f40",
   "metadata": {},
   "source": [
    "### Еще функции `.collect()`, `.take()`, `.toPandas()`\n",
    "\n",
    "* `.collect()` - получает список строк\n",
    "* `.take()` - получает заданное число строк\n",
    "* `.toPandas()` - формирует pandas DataFrame от полученных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0c90ed84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3.19 s\n",
      "Wall time: 6.39 s\n"
     ]
    }
   ],
   "source": [
    "# Строим план вычислений\n",
    "filter_df_one = df.filter(df.Survived == 0)\n",
    "\n",
    "%time result_rows = filter_df_one.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f7dfd6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "618319\n",
      "Row(PassengerId=1311, Survived=0, Pclass=3, Name='Name1311, Col. Surname1311', Sex='male', Age=29.0, SibSp=0, Parch=0, Ticket='223596', Fare=10.193096706320182, Cabin=None, Embarked='S')\n"
     ]
    }
   ],
   "source": [
    "print(len(result_rows))\n",
    "print(result_rows[0])\n",
    "\n",
    "del result_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e532dfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(PassengerId=1311, Survived=0, Pclass=3, Name='Name1311, Col. Surname1311', Sex='male', Age=29.0, SibSp=0, Parch=0, Ticket='223596', Fare=10.193096706320182, Cabin=None, Embarked='S'),\n",
       " Row(PassengerId=1312, Survived=0, Pclass=3, Name='Name1312, Mr. Surname1312', Sex='male', Age=20.0, SibSp=0, Parch=0, Ticket='54636', Fare=12.02941641147422, Cabin='C83', Embarked='C')]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_rows_2 = filter_df_one.take(10)\n",
    "print(len(result_rows_2))\n",
    "result_rows_2[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54e77b7",
   "metadata": {},
   "source": [
    "## Способы создания DataFrame'ов\n",
    "\n",
    "Подробней прочитать можно: [тут](https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html#DataFrame-Creation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ed683d",
   "metadata": {},
   "source": [
    "### Добавление нового столбца"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb04e0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+------------------+-----+--------+----------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|              Fare|Cabin|Embarked|FamilySize|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+------------------+-----+--------+----------+\n",
      "|       1310|       1|     1|Name1310, Miss. S...|female|NULL|    0|    0|SOTON/O2 3101272| 76.76016504643573| NULL|       C|         0|\n",
      "|       1311|       0|     3|Name1311, Col. Su...|  male|29.0|    0|    0|          223596|10.193096706320182| NULL|       S|         0|\n",
      "|       1312|       0|     3|Name1312, Mr. Sur...|  male|20.0|    0|    0|           54636| 12.02941641147422|  C83|       C|         0|\n",
      "|       1313|       0|     3|Name1313, Mr. Sur...|  male|27.0|    0|    0|        PC 17760|13.429447862759872| NULL|       S|         0|\n",
      "|       1314|       0|     3|Name1314, Mr. Sur...|  male|32.0|    0|    0|          364512| 4.840769450167068|  E33|       C|         0|\n",
      "|       1315|       1|     3|Name1315, Master....|female| 0.0|    0|    0|           29750|14.805817148933464| NULL|       S|         0|\n",
      "|       1316|       0|     1|Name1316, Mr. Sur...|  male|NULL|    0|    0|      C.A. 24579|   115.80634419925| NULL|       C|         0|\n",
      "|       1317|       1|     2|Name1317, Miss. S...|female|28.0|    0|    1|          113781|27.950047997073444| NULL|       S|         1|\n",
      "|       1318|       0|     1|Name1318, Mr. Sur...|  male|NULL|    0|    0|          250652|29.467202626662697| NULL|       S|         0|\n",
      "|       1319|       0|     2|Name1319, Mr. Sur...|  male|32.0|    0|    0|           17464|               0.0| NULL|       S|         0|\n",
      "|       1320|       1|     3|Name1320, Miss. S...|female|23.0|    1|    0|            7534|20.822545765670117| NULL|       S|         1|\n",
      "|       1321|       0|     3|Name1321, Mr. Sur...|  male|NULL|    0|    0|            2689|  4.79782744017412| NULL|       C|         0|\n",
      "|       1322|       1|     3|Name1322, Miss. S...|female|NULL|    1|    0|     S.O./P.P. 3|12.462809150380991| NULL|       S|         1|\n",
      "|       1323|       1|     1|Name1323, Mr. Sur...|  male|52.0|    0|    0|      A.5. 18509| 36.84335154107275|  E25|       S|         0|\n",
      "|       1324|       0|     3|Name1324, Mr. Sur...|  male|46.0|    0|    0|          239854|               0.0| NULL|       S|         0|\n",
      "|       1325|       0|     3|Name1325, Mr. Sur...|  male|29.0|    0|    0|          248727|12.274214426395227| NULL|       S|         0|\n",
      "|       1326|       0|     3|Name1326, Mr. Sur...|  male|41.0|    0|    0|            2694|13.845328222553944| NULL|       S|         0|\n",
      "|       1327|       1|     2|Name1327, Master....|female| 4.0|    0|    2|      A/5. 10482| 26.71240152233816| NULL|       S|         2|\n",
      "|       1328|       0|     3|Name1328, Mr. Sur...|  male|27.0|    0|    0|      C.A. 34651|17.263996279241812| NULL|       S|         0|\n",
      "|       1329|       0|     3|Name1329, Mr. Sur...|  male|NULL|    1|    1|          315082| 20.56614114030044| NULL|       S|         2|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+------------------+-----+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"FamilySize\", df.Parch + df.SibSp).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4faba527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema() # Можно заметить, что схема исходного источника данных осталась без изменений\n",
    "# т.е df.withColumn(\"FamilySize\", df.Parch + df.SibSp) также создает новый DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ffb9460",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"FamilySize\", df.Parch + df.SibSp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14385041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      " |-- FamilySize: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema() # Можно заметить появление FamilySize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e528486b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [PassengerId#17, Survived#18, Pclass#19, Name#20, Sex#21, Age#22, SibSp#23, Parch#24, Ticket#25, Fare#26, Cabin#27, Embarked#28, (Parch#24 + SibSp#23) AS FamilySize#121]\n",
      "+- FileScan csv [PassengerId#17,Survived#18,Pclass#19,Name#20,Sex#21,Age#22,SibSp#23,Parch#24,Ticket#25,Fare#26,Cabin#27,Embarked#28] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/dev/mlRes/BigDataTech/seminars/sem_1/data/huge_1M_titanic.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<PassengerId:int,Survived:int,Pclass:int,Name:string,Sex:string,Age:double,SibSp:int,Parch:...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Можно заметить, как стало стал отличаться физический план\n",
    "df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1579f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+------------------+-----+--------+----------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|              Fare|Cabin|Embarked|FamilySize|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+------------------+-----+--------+----------+\n",
      "|       1310|       1|     1|Name1310, Miss. S...|female|NULL|    0|    0|SOTON/O2 3101272| 76.76016504643573| NULL|       C|         0|\n",
      "|       1311|       0|     3|Name1311, Col. Su...|  male|29.0|    0|    0|          223596|10.193096706320182| NULL|       S|         0|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+------------------+-----+--------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9376973",
   "metadata": {},
   "source": [
    "## Способы записи (сохранение DataFrame)\n",
    "\n",
    "Позволяет записать полученный DataFrame в любой из поддерживаемых форматов файлов. Подробней: [тут](https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html#Getting-Data-In/Out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccebd6ff",
   "metadata": {},
   "source": [
    "## Группировка и агрегация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b819fbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[Pclass: string, count(PassengerId): bigint]\n"
     ]
    }
   ],
   "source": [
    "# Схоже с pandas, только отличе в том, что данные не хранятся все в памяти\n",
    "\n",
    "# Определить количество людей в каждом из классов\n",
    "result_gr_1 = df.groupby(df.Pclass).agg(\n",
    "    F.count(df.PassengerId)\n",
    ")\n",
    "\n",
    "# Также строит план вычислений\n",
    "print(result_gr_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de017641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|Pclass|count(PassengerId)|\n",
      "+------+------------------+\n",
      "|     3|            553319|\n",
      "|     1|            242069|\n",
      "|     2|            204612|\n",
      "+------+------------------+\n",
      "\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 1.01 s\n"
     ]
    }
   ],
   "source": [
    "# Вычислить\n",
    "%time result_gr_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "76e0cd1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Pclass: int, count: bigint]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Тоже строится план, а не сразу вычисляется\n",
    "df.groupby(df.Pclass).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "beb5429e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|Pclass| count|\n",
      "+------+------+\n",
      "|     1|242069|\n",
      "|     3|553319|\n",
      "|     2|204612|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby(df.Pclass).count().show()\n",
    "# df.groupby(df.Pclass).avg(\"PassengerId\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2ab1805e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+\n",
      "|Pclass|Survived| count|\n",
      "+------+--------+------+\n",
      "|     1|       0| 90668|\n",
      "|     3|       1|132666|\n",
      "|     1|       1|151401|\n",
      "|     2|       1| 97614|\n",
      "|     2|       0|106998|\n",
      "|     3|       0|420653|\n",
      "+------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Можно выполнять группировку по нескольким столбцам\n",
    "\n",
    "df.groupby(df.Pclass, df.Survived).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71424ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------------------+-----------+\n",
      "|Pclass|Survived|          mean_age|psngr_count|\n",
      "+------+--------+------------------+-----------+\n",
      "|     1|       1| 34.14365493120401|     151401|\n",
      "|     2|       0| 32.45753543095491|     106998|\n",
      "|     3|       1|23.244919110926855|     132666|\n",
      "|     3|       0|  27.0155267639002|     420653|\n",
      "|     2|       1| 26.09276820107242|      97614|\n",
      "|     1|       0|40.196781121788696|      90668|\n",
      "+------+--------+------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Поддержка нескольких функций агрегации. Также можно установить псевдонимы\n",
    "df.groupBy(df.Pclass, df.Survived).agg(\n",
    "    F.avg(df.Age).alias(\"mean_age\"),\n",
    "    F.count(df.PassengerId).alias(\"psngr_count\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb79190",
   "metadata": {},
   "source": [
    "### Сортировка: `.orderBy()`\n",
    "\n",
    "Одна из самых затратных операций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93388dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+------------------+\n",
      "|Pclass|passanger_count|          mean_age|\n",
      "+------+---------------+------------------+\n",
      "|     1|         151401| 34.14365493120401|\n",
      "|     2|          97614| 26.09276820107242|\n",
      "|     3|         132666|23.244919110926855|\n",
      "+------+---------------+------------------+\n",
      "\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 465 ms\n",
      "+------+---------------+------------------+\n",
      "|Pclass|passanger_count|          mean_age|\n",
      "+------+---------------+------------------+\n",
      "|     2|          97614| 26.09276820107242|\n",
      "|     3|         132666|23.244919110926855|\n",
      "|     1|         151401| 34.14365493120401|\n",
      "+------+---------------+------------------+\n",
      "\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 446 ms\n"
     ]
    }
   ],
   "source": [
    "# Можно, например, отсортировать финальный результат, группировки или какой-либо выборки, фильтрации\n",
    "\n",
    "# Без сортировки\n",
    "%time df.filter(df.Survived==1).groupby(df.Pclass).agg(F.count(df.PassengerId).alias(\"passanger_count\"), F.mean(df.Age).alias(\"mean_age\")).show()\n",
    "\n",
    "# С сортировкой\n",
    "%time df.filter(df.Survived==1).groupby(df.Pclass).agg(F.count(df.PassengerId).alias(\"passanger_count\"), F.mean(df.Age).alias(\"mean_age\")).orderBy(\"passanger_count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42c7d5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------+----+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|    Ticket|Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------+----+-----+--------+\n",
      "|     962962|       0|     2|Name962962, Mr. S...|  male|51.0|    0|    0|     13502| 0.0|  C78|       S|\n",
      "|       1718|       1|     1|Name1718, Miss. S...|female|23.0|    0|    0|A.5. 18509| 0.0| C148|       C|\n",
      "|     788890|       0|     2|Name788890, Mr. S...|  male|40.0|    0|    0|    113784| 0.0| NULL|       S|\n",
      "|       2038|       0|     2|Name2038, Mr. Sur...|  male|27.0|    0|    0|     13568| 0.0|  C87|       S|\n",
      "|     266498|       0|     3|Name266498, Miss....|female|15.0|    1|    0|    349909| 0.0| NULL|       S|\n",
      "|       1719|       0|     3|Name1719, Mr. Sur...|  male|NULL|    0|    0|    234686| 0.0| NULL|       S|\n",
      "|     788885|       0|     1|Name788885, Mr. S...|  male|68.0|    0|    0|    347088| 0.0|  D11|       S|\n",
      "|       1683|       0|     2|Name1683, Mr. Sur...|  male|45.0|    0|    0|    349257| 0.0| NULL|       S|\n",
      "|     353422|       1|     1|Name353422, Mr. S...|  male|23.0|    1|    0|  PC 17757| 0.0|  B78|       S|\n",
      "|       1324|       0|     3|Name1324, Mr. Sur...|  male|46.0|    0|    0|    239854| 0.0| NULL|       S|\n",
      "|     788904|       1|     3|Name788904, Major...|  male| 4.0|    0|    0|    243880| 0.0| NULL|       C|\n",
      "|       2032|       1|     3|Name2032, Mr. Sur...|  male|41.0|    0|    0|    234818| 0.0| NULL|       S|\n",
      "|     266662|       0|     3|Name266662, Mr. S...|  male|39.0|    0|    0|    113794| 0.0| NULL|       S|\n",
      "|       1752|       1|     3|Name1752, Mr. Sur...|  male|NULL|    0|    0|    349237| 0.0| NULL|       S|\n",
      "|     788620|       1|     3|Name788620, Mr. S...|  male|NULL|    0|    0|     27267| 0.0| NULL|       C|\n",
      "|       1345|       1|     1|Name1345, Miss. S...|female|NULL|    0|    0|      5727| 0.0| NULL|       C|\n",
      "|     527638|       1|     1|Name527638, Mr. S...|  male| 3.0|    0|    1|    349218| 0.0|  A36|       C|\n",
      "|       1785|       0|     1|Name1785, Mr. Sur...|  male|NULL|    0|    0|  A/5. 851| 0.0| NULL|       C|\n",
      "|     788639|       0|     1|Name788639, Maste...|female| 5.0|    1|    2|    315153| 0.0| NULL|       S|\n",
      "|       1457|       1|     3|Name1457, Mr. Sur...|  male|22.0|    0|    0|    110413| 0.0| NULL|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------+----+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 491 ms\n"
     ]
    }
   ],
   "source": [
    "# Например, сколько времени было затрачено на выполнение сортировки\n",
    "%time df.orderBy(df.Fare).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4acc3d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+------------------+----+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|            Ticket|Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+------------------+----+-----+--------+\n",
      "|     963134|       0|     2|Name963134, Mr. S...|  male|55.0|    0|    0|            347077| 0.0| NULL|       S|\n",
      "|     440547|       1|     1|Name440547, Miss....|female|66.0|    0|    1|              3460| 0.0| NULL|       C|\n",
      "|       1665|       1|     3|Name1665, Miss. S...|female|34.0|    1|    1|            349225| 0.0| NULL|       S|\n",
      "|     440700|       1|     1|Name440700, Mr. S...|  male|27.0|    1|    0|SOTON/O.Q. 3101305| 0.0|  C49|       C|\n",
      "|     527618|       0|     2|Name527618, Mr. S...|  male|34.0|    0|    0|            315093| 0.0| NULL|       S|\n",
      "|     440515|       0|     1|Name440515, Mr. S...|  male|33.0|    0|    0|         A/5 21174| 0.0| E121|       S|\n",
      "|       1955|       1|     3|Name1955, Mr. Sur...|  male|20.0|    0|    0|            250649| 0.0| NULL|       C|\n",
      "|     440670|       1|     3|Name440670, Mr. S...|  male|37.0|    0|    0|             29751| 0.0|  B49|       S|\n",
      "|      92197|       1|     1|Name92197, Mr. Su...|  male|47.0|    0|    0|            336439| 0.0| NULL|       C|\n",
      "|     440360|       0|     2|Name440360, Mr. S...|  male|36.0|    0|    0|            C 7075| 0.0| NULL|       S|\n",
      "|       1517|       1|     1|Name1517, Mrs. Su...|female|NULL|    1|    1|     S.O./P.P. 751| 0.0| NULL|       C|\n",
      "|     440419|       0|     1|Name440419, Mr. S...|  male|62.0|    1|    1|            349208| 0.0|  D37|       C|\n",
      "|     527787|       0|     2|Name527787, Mr. S...|  male|40.0|    0|    0|            349210| 0.0|  D48|       S|\n",
      "|     440352|       1|     1|Name440352, Miss....|female|35.0|    0|    0|            367226| 0.0| NULL|       C|\n",
      "|       1949|       0|     2|Name1949, Mr. Sur...|  male|50.0|    0|    0|            386525| 0.0| NULL|       S|\n",
      "|     440438|       0|     1|Name440438, Mr. S...|  male|62.0|    0|    0|            365222| 0.0| NULL|       C|\n",
      "|     788793|       1|     3|Name788793, Miss....|female|NULL|    1|    1|            350404| 0.0| NULL|       S|\n",
      "|     440382|       0|     1|Name440382, Mr. S...|  male|63.0|    0|    0|              2649| 0.0| NULL|       S|\n",
      "|       1683|       0|     2|Name1683, Mr. Sur...|  male|45.0|    0|    0|            349257| 0.0| NULL|       S|\n",
      "|     440580|       0|     3|Name440580, Mr. S...|  male| 5.0|    5|    2|            349245| 0.0| NULL|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+------------------+----+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 540 ms\n"
     ]
    }
   ],
   "source": [
    "%time df.sort(df.Fare).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebabfd48",
   "metadata": {},
   "source": [
    "## Оконные функции\n",
    "\n",
    "be later ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688b9e6c",
   "metadata": {},
   "source": [
    "## SQL \n",
    "\n",
    "Spark поддерживает работу с SQL, можно писать запросы на привычном SQL, как при работе с базой данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "864caa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Необходимо зарегистрировать df как таблицу (дать имя, например: tableOne)\n",
    "df.createOrReplaceTempView(\"tableOne\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ca7d263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Также строится план вычислений\n",
    "calculate_plan_sql = spark.sql(\"SELECT * FROM tableOne\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4895620b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+------------------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|              Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+------------------+-----+--------+\n",
      "|       1310|       1|     1|Name1310, Miss. S...|female|NULL|    0|    0|SOTON/O2 3101272| 76.76016504643573| NULL|       C|\n",
      "|       1311|       0|     3|Name1311, Col. Su...|  male|29.0|    0|    0|          223596|10.193096706320182| NULL|       S|\n",
      "|       1312|       0|     3|Name1312, Mr. Sur...|  male|20.0|    0|    0|           54636| 12.02941641147422|  C83|       C|\n",
      "|       1313|       0|     3|Name1313, Mr. Sur...|  male|27.0|    0|    0|        PC 17760|13.429447862759872| NULL|       S|\n",
      "|       1314|       0|     3|Name1314, Mr. Sur...|  male|32.0|    0|    0|          364512| 4.840769450167068|  E33|       C|\n",
      "|       1315|       1|     3|Name1315, Master....|female| 0.0|    0|    0|           29750|14.805817148933464| NULL|       S|\n",
      "|       1316|       0|     1|Name1316, Mr. Sur...|  male|NULL|    0|    0|      C.A. 24579|   115.80634419925| NULL|       C|\n",
      "|       1317|       1|     2|Name1317, Miss. S...|female|28.0|    0|    1|          113781|27.950047997073444| NULL|       S|\n",
      "|       1318|       0|     1|Name1318, Mr. Sur...|  male|NULL|    0|    0|          250652|29.467202626662697| NULL|       S|\n",
      "|       1319|       0|     2|Name1319, Mr. Sur...|  male|32.0|    0|    0|           17464|               0.0| NULL|       S|\n",
      "|       1320|       1|     3|Name1320, Miss. S...|female|23.0|    1|    0|            7534|20.822545765670117| NULL|       S|\n",
      "|       1321|       0|     3|Name1321, Mr. Sur...|  male|NULL|    0|    0|            2689|  4.79782744017412| NULL|       C|\n",
      "|       1322|       1|     3|Name1322, Miss. S...|female|NULL|    1|    0|     S.O./P.P. 3|12.462809150380991| NULL|       S|\n",
      "|       1323|       1|     1|Name1323, Mr. Sur...|  male|52.0|    0|    0|      A.5. 18509| 36.84335154107275|  E25|       S|\n",
      "|       1324|       0|     3|Name1324, Mr. Sur...|  male|46.0|    0|    0|          239854|               0.0| NULL|       S|\n",
      "|       1325|       0|     3|Name1325, Mr. Sur...|  male|29.0|    0|    0|          248727|12.274214426395227| NULL|       S|\n",
      "|       1326|       0|     3|Name1326, Mr. Sur...|  male|41.0|    0|    0|            2694|13.845328222553944| NULL|       S|\n",
      "|       1327|       1|     2|Name1327, Master....|female| 4.0|    0|    2|      A/5. 10482| 26.71240152233816| NULL|       S|\n",
      "|       1328|       0|     3|Name1328, Mr. Sur...|  male|27.0|    0|    0|      C.A. 34651|17.263996279241812| NULL|       S|\n",
      "|       1329|       0|     3|Name1329, Mr. Sur...|  male|NULL|    1|    1|          315082| 20.56614114030044| NULL|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+------------------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "calculate_plan_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cabc6c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o23.sql.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.liftedTree1$1(InMemoryCatalog.scala:122)\r\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.createDatabase(InMemoryCatalog.scala:119)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:159)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)\r\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$catalog$1(BaseSessionStateBuilder.scala:154)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:123)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:123)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.databaseExists(SessionCatalog.scala:321)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.requireDbExists(SessionCatalog.scala:251)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableRawMetadata(SessionCatalog.scala:546)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableMetadata(SessionCatalog.scala:532)\r\n\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.loadTable(V2SessionCatalog.scala:75)\r\n\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.getTable(CatalogV2Util.scala:363)\r\n\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:337)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$5(Analyzer.scala:1315)\r\n\tat scala.Option.orElse(Option.scala:447)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1311)\r\n\tat scala.Option.orElse(Option.scala:447)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1296)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1153)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1117)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\r\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\r\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.Aggregate.mapChildren(basicLogicalOperators.scala:1151)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\r\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\r\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.Sort.mapChildren(basicLogicalOperators.scala:920)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1117)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1076)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\r\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\r\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\r\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\r\n\tat scala.collection.immutable.List.foreach(List.scala:431)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)\r\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\r\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_one \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSELECT Pclass, COUNT(PassengerId) as count_pssng FROM tableOne GROUP BY Pclass ORDER BY count_pssng\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\dev\\mlRes\\mlenv\\lib\\site-packages\\pyspark\\sql\\session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[1;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[0;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[0;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[0;32m   1630\u001b[0m         )\n\u001b[1;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\dev\\mlRes\\mlenv\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\dev\\mlRes\\mlenv\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\dev\\mlRes\\mlenv\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o23.sql.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.liftedTree1$1(InMemoryCatalog.scala:122)\r\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.createDatabase(InMemoryCatalog.scala:119)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:159)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)\r\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$catalog$1(BaseSessionStateBuilder.scala:154)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:123)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:123)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.databaseExists(SessionCatalog.scala:321)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.requireDbExists(SessionCatalog.scala:251)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableRawMetadata(SessionCatalog.scala:546)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableMetadata(SessionCatalog.scala:532)\r\n\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.loadTable(V2SessionCatalog.scala:75)\r\n\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.getTable(CatalogV2Util.scala:363)\r\n\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:337)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$5(Analyzer.scala:1315)\r\n\tat scala.Option.orElse(Option.scala:447)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1311)\r\n\tat scala.Option.orElse(Option.scala:447)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1296)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1153)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1117)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\r\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\r\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.Aggregate.mapChildren(basicLogicalOperators.scala:1151)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\r\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\r\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.Sort.mapChildren(basicLogicalOperators.scala:920)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1117)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1076)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\r\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\r\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\r\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\r\n\tat scala.collection.immutable.List.foreach(List.scala:431)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)\r\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\r\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT Pclass, COUNT(PassengerId) as count_pssng FROM tableOne GROUP BY Pclass ORDER BY count_pssng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48032118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+------------------+\n",
      "|PassengerId|                Name|              Fare|\n",
      "+-----------+--------------------+------------------+\n",
      "|       1310|Name1310, Miss. S...| 76.76016504643573|\n",
      "|       1311|Name1311, Col. Su...|10.193096706320182|\n",
      "|       1312|Name1312, Mr. Sur...| 12.02941641147422|\n",
      "|       1313|Name1313, Mr. Sur...|13.429447862759872|\n",
      "|       1314|Name1314, Mr. Sur...| 4.840769450167068|\n",
      "|       1315|Name1315, Master....|14.805817148933464|\n",
      "|       1316|Name1316, Mr. Sur...|   115.80634419925|\n",
      "|       1317|Name1317, Miss. S...|27.950047997073444|\n",
      "|       1318|Name1318, Mr. Sur...|29.467202626662697|\n",
      "|       1319|Name1319, Mr. Sur...|               0.0|\n",
      "|       1320|Name1320, Miss. S...|20.822545765670117|\n",
      "|       1321|Name1321, Mr. Sur...|  4.79782744017412|\n",
      "|       1322|Name1322, Miss. S...|12.462809150380991|\n",
      "|       1323|Name1323, Mr. Sur...| 36.84335154107275|\n",
      "|       1324|Name1324, Mr. Sur...|               0.0|\n",
      "|       1325|Name1325, Mr. Sur...|12.274214426395227|\n",
      "|       1326|Name1326, Mr. Sur...|13.845328222553944|\n",
      "|       1327|Name1327, Master....| 26.71240152233816|\n",
      "|       1328|Name1328, Mr. Sur...|17.263996279241812|\n",
      "|       1329|Name1329, Mr. Sur...| 20.56614114030044|\n",
      "+-----------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT PassengerId, Name, Fare FROM tableOne\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
